---
layout: clubs
content_id: "publications"
title: "Die Veröffentlichungen"
---

<div class="d-flex flex-column">
    <h1 class="display-4 mb-3 align-self-center align-self-md-start">Veröffentlichungen</h1>
</div>

<style type="text/css">
    @media screen and (min-width:768px) {
        .card {
            max-width: 100%;
        }
    }

    @media screen and (max-width:767px) {
        .card {
            min-width: 80%;
        }
    }
</style>

<div class="d-flex flex-column">
    <h2 class="my-3 align-self-center align-self-md-start">Zeitschriftenbeiträge</h2>
    <div class="card-deck d-flex flex-wrap justify-content-center justify-content-md-start">
        <div class="card mb-md-3 mb-3 w-50">
            <div class="card-header">
                <h4 class="card-title">
                    <a href="http://link.springer.com/article/10.1007/s13222-016-0244-3">PubPsych: A Powerful Research Tool Providing Access to a Broad Supranational Body of Psychological Knowledge</a>
                </h4>
                <h6 class="card-subtitle mb-2 text-muted">Erich Weichselgartner & Christiane Baier & Roland Ramthun</h6></div>
            <div class="card-body">

                <p class="card-text">We discuss the motivation for creating the system, design decisions, connected problems and our solutions, especially concerning multilingual information.</p>
            </div>
            <div class="card-footer">
                <small class="text-muted">Veröffentlicht: März 2017</small>
            </div>
        </div>
    </div>
</div>

<div class="d-flex flex-column">
    <h2 class="my-3 align-self-center align-self-md-start">Sonstige</h2>
    <div class="card-deck d-flex flex-wrap justify-content-center justify-content-md-start">
        <div class="card mb-md-3 mb-3">
            <div class="card-header"><h4 class="card-title"><a href="/assets/publications/other/MSc_thesis_AdamVarga.pdf">Master's thesis: Domain Adaptation for Multilingual Neural Machine Translation
                    </a></h4>
                <h6 class="card-subtitle mb-2 text-muted">Ádám Csaba Varga</h6></div>
            <div class="card-body">
                <p class="card-text">Neural machine translation (NMT) is currently considered the state-of-the-art for language pairs with vast amounts of parallel data. In this thesis project, we utilize such systems to provide translations between four languages in the psychology domain, where the biggest challenge is posed by in-domain data scarcity. Therefore, the emphasis of the research is laid on exploring domain adaptation methods in this scenario. We first propose a system for automatically building in-domain adaptation corpora by extracting parallel sentence pairs from comparable articles of Wikipedia. To this end, we use supervised classification and regression methods trained on NMT context vector similarities and complementary textual similarity features. We find that the best method for our purposes is a regression model trained on continuous similarity labels. We rerank the extracted candidates by their similarity feature averages and use the top-N partitions as adaptation corpora. In the second part of the thesis we thoroughly examine multilingual domain adaptation by transfer learning with respect to the adaptation data quality, size, and domain. With clean parallel in-domain adaptation data we achieve significant improvements for most translation directions, including ones with no adaptation data, while the automatically extracted corpora prove beneficial mostly for language pairs with no clean in-domain adaptation set. Particularly in these latter cases, the combination of the two adaptation corpora yields further improvements. We also explore the possibilities of reranking N -best translation lists with in-domain language models and similarity features. We conclude that adapted systems produce candidates that can result in a higher improvement in translation performance than the ones of unadapted models, and that remarkable improvements can be achieved by similarity-based reranking methods.
                </p>
            </div>
            <div class="card-footer">
                <small class="text-muted">Veröffentlicht: August 2017</small>
            </div>
        </div>
        <div class="card mb-md-3 mb-3">
            <div class="card-header"><h4 class="card-title"><a href="/assets/publications/other/Bonet_Bedeno_semEval17.pdf">Lump at SemEval-2017 Task 1: Towards an Interlingua Semantic Similarity</a></h4>
                <h6 class="card-subtitle mb-2 text-muted">Cristina España-Bonet &amp; Alberto Barrón-Cedeño</h6></div>
            <div class="card-body">
                <p class="card-text">This is the Lump team participation at SemEval 2017 Task 1 on Semantic Textual Similarity. Our supervised model relies on features which are multilingual or interlingual in nature. We include lexical similarities, cross-language explicit semantic analysis, internal representations of multilingual neural networks and interlingual word embeddings. Our representations allow to use large datasets in language pairs with many instances to better classify instances in smaller language pairs avoiding the necessity of translating into a single language. Hence we can deal with all the languages in the task: Arabic, English, Spanish, and Turkish.
                </p>
            </div>
            <div class="card-footer">
                <small class="text-muted">Veröffentlicht: August 2017</small>
            </div>
        </div>
    </div>
    <div class="card-deck d-flex flex-wrap justify-content-center justify-content-md-start">
        <div class="card mb-md-3 mb-3">
        <div class="card-header"><h4 class="card-title"><a href="/assets/publications/other/Espana_vanGenabith_IWSLT17.pdf">Going beyond zero-shot MT: combining phonological, morphological and semantic factors. The UdS-DFKI System at IWSLT 2017</a></h4>
            <h6 class="card-subtitle mb-2 text-muted">Cristina España-Bonet &amp; Josef van Genabith</h6></div>
        <div class="card-body">
            <p class="card-text">This paper describes the UdS-DFKI participation to the multilingual task of the IWSLT Evaluation 2017. Our approach is based on factored multilingual neural translation systems following the small data and zero-shot training conditions. Our systems are designed to fully exploit multilinguality by including factors that increase the number of common elements among languages such as phonetic coarse encodings and synsets, besides shallow part-of-speech tags, stems and lemmas. Document level information is also considered by including the topic of every document. This approach improves a baseline without any additional factor for all the language pairs and even allows beyond-zero-shot translation. That is, the translation from unseen languages is possible thanks to the common elements &mdash;especially synsets in our models&mdash; among languages.
            </p>
        </div>
        <div class="card-footer">
            <small class="text-muted">Veröffentlicht: Dezember 2017</small>
        </div>
        </div>
        <div class="card mb-md-3 mb-3">
        <div class="card-header"><h4 class="card-title"><a href="/assets/publications/other/Espana_EtAl_IEEE">An Empirical Analysis of NMT-Derived Interlingual Embeddings and their Use in Parallel Sentence Identification</a></h4>
            <h6 class="card-subtitle mb-2 text-muted">Cristina España-Bonet, Ádám Csaba Varga, Alberto Barrón-Cedeño &amp; Josef van Genabith</h6></div>
        <div class="card-body">
            <p class="card-text">End-to-end neural machine translation has overtaken statistical machine translation in terms of translation quality for some language pairs, specially those with a large amount of parallel data available. Beside this palpable improvement, neural networks embrace several new properties. A single system can be trained to translate between many languages at almost no additional cost other than training time. Furthermore, internal representations learned by the network serve as a new semantic representation of words -or sentences- which, unlike standard word embeddings, are learned in an essentially bilingual or even multilingual context. In view of these properties, the contribution of the present work is two-fold. First, we systematically study the context vectors, i.e. output of the encoder, and their prowess as an interlingua representation of a sentence. Their quality and effectiveness are assessed by similarity measures across translations, semantically related, and semantically unrelated sentence pairs. Second, and as extrinsic evaluation of the first point, we identify parallel sentences in comparable corpora, obtaining an F1=98.2% on data from a shared task when using only context vectors. F1 reaches 98.9% when complementary similarity measures are used.
            </p>
        </div>
        <div class="card-footer">
            <small class="text-muted">Veröffentlicht: Dezember 2017</small>
        </div>
        </div>
    </div>
    <div class="card-deck d-flex flex-wrap justify-content-center justify-content-md-start">
        <div class="card mb-md-3 mb-3">
            <div class="card-header"><h4 class="card-title">Master's thesis: Query Log Analysis of Information Search Behaviour in the Psychological Domain: a Case Study of PubPsych</h4>
                <h6 class="card-subtitle mb-2 text-muted">Jie Yin</h6></div>
            <div class="card-body">
                <p class="card-text">This study focused on analysing user search behaviours in the psychological domain, as well as comparing with web search engines, other specialised search engines and former study in the psychological domain. The specific research questions are related to session length and query length, the language of sessions and queries, language switch, query reformulation patterns and query categories, term frequency and term distribution and search failure.
                </p>
            </div>
            <div class="card-footer">
                <small class="text-muted">Veröffentlicht: Juni 2017</small>
            </div>
        </div>
    </div>
</div>